<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Matt O'Brien (dot) Me - deep learning</title><link href="http://mattobrien.me/" rel="alternate"></link><link href="http://mattobrien.me/feeds/deep-learning.atom.xml" rel="self"></link><id>http://mattobrien.me/</id><updated>2017-10-09T00:00:00-07:00</updated><entry><title>Deep Learning for Sport Wagering Part 3 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-3-of-3.html" rel="alternate"></link><published>2017-10-09T00:00:00-07:00</published><updated>2017-10-09T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-10-09:/deep-learning-for-sport-wagering-part-3-of-3.html</id><summary type="html">&lt;p&gt;Predicting&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 2: Predicting&lt;/h4&gt;
&lt;p&gt;I considered two major betting strategies during this final phase of the project. They are as follows:  &lt;/p&gt;
&lt;p&gt;1) If &lt;br&gt;
model probability &amp;gt; some decision threshold, and&lt;br&gt;
model probability &amp;gt; sportsbook probability&lt;br&gt;
then place bet  &lt;/p&gt;
&lt;p&gt;2) If model probability &amp;gt; some decision threshold
then place bet&lt;/p&gt;
&lt;p&gt;The first strategy is a more conservative approach in one sense. With this perspective, we care if we feel like we have an edge on the casino or sportsbook. &lt;/p&gt;
&lt;p&gt;Allow me a quick foray into the structure of gambling. Sportsbook odds are set, not by information, but by popular sentiment. If some Boxer A is favored by the public, then the sportsbook will consider Boxer A to be more likely to win. Thus, my model is attempting to answer the question, "When does prediction based on historic data result in a confidence higher than the confidence represented by public perception?".&lt;/p&gt;
&lt;p&gt;This first strategy also has a built in safeguard. If the sportsbook places some Boxer A at a 10% chance of winning, and the model predicts an 11% chance of winning, then a bet will not placed (assuming the decision threshold is greater than 10%, which, it is).&lt;/p&gt;
&lt;p&gt;Moving on, the second strategy isn't concerned with the sportsbook's behavior at all; merely concerned with the strength of it's (the model's) own predictions. &lt;/p&gt;
&lt;p&gt;Let's examine both strategies, and see how they played out.   &lt;/p&gt;
&lt;h4&gt;Strategy 1&lt;/h4&gt;
&lt;p&gt;To be implemented, the first strategy required a data acquisition step first. Historic sportsbook odds needed to be collected. This is for the purpose of compairing the probability of a win assigned by a sportsbook to the probability of a win assigned by the model.  &lt;/p&gt;
&lt;p&gt;In boxing, the bookmaker's odds come structured into a form which is referred to as the &lt;a href="https://en.wikipedia.org/wiki/Odds#Moneyline_odds"&gt;moneyline&lt;/a&gt;. The moneyline is a little confusing at first. Generally, one fighter who considered favored to win is assigned a negative number. The other fighter is considered the underdog, and is assigned a positive number. &lt;/p&gt;
&lt;p&gt;The best way to remember how to read the moneyline is to always start with the image of a one-hundred dollar bill in your mind.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A negative number shows how much money you need to bet to win a profit of $100.  &lt;/li&gt;
&lt;li&gt;A positive number shows how much profit a winning wager of $100 would yield.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if the moneyline has Boxer A at -130, we know that Boxer A is expected to win. Further, you know you'd have to place a $130 bet on this fighter to win $100.&lt;br&gt;
For Boxer B, the moneyline might have them set at +110. This mean Boxer B is the underdog, and if you placed a $100 bet on this fighter, you'd win $110.  &lt;/p&gt;
&lt;p&gt;The actual numeric values (-130 and +110, on this example above), can be converted to what are referred to as &lt;a href="https://www.sbo.net/strategy/implied-probability/"&gt;implied probabilities&lt;/a&gt; (more about these in a second).&lt;/p&gt;
&lt;p&gt;Implied probability =   ( - ( 'negative' moneyline value ) ) / ( - ( 'negative' moneyline value ) ) + 100&lt;br&gt;
and&lt;br&gt;
Implied probability =   100 / ( 'positive' moneyline value + 100 )  &lt;/p&gt;
&lt;p&gt;Thus, -130 is converted to %56.5, and +110 is converted to %50.  &lt;/p&gt;
&lt;p&gt;But what is an implied probability anyway?&lt;/p&gt;
&lt;p&gt;Implied probability is our usual notion of probability which has actually been modified by what is called either &lt;a href="https://en.wikipedia.org/wiki/Vigorish"&gt;vigorish, or juice&lt;/a&gt;. Both of these terms refer to a built-in modification, by the bookmaker, of the true odds. The modification shifts the moneyline is such a way that the sportsbooks can never ultimately lose money. Usually, the vig amounts to 20 points. It's basically the casino's cut. Fortunately, it's easy to remove the vigorish using this simple formula:  &lt;/p&gt;
&lt;p&gt;Take one of your implied probability. Divide it by the sum of both of your implied probabilities.  &lt;/p&gt;
&lt;p&gt;For example:  &lt;/p&gt;
&lt;p&gt;Actual probability = Implied probability A / (Implied probability A + Implied probability B)  &lt;/p&gt;
&lt;p&gt;With the math settled, I began searching for a set of historic moneylines for records which I could use in my test set. Using a variety of sources (including laborious searching of the Wayback Machine), I collected a set of 690 moneylines. &lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry><entry><title>Deep Learning for Sport Wagering Part 2 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-2-of-3.html" rel="alternate"></link><published>2017-09-16T00:00:00-07:00</published><updated>2017-09-16T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-09-16:/deep-learning-for-sport-wagering-part-2-of-3.html</id><summary type="html">&lt;p&gt;Modeling&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 2: Modeling&lt;/h4&gt;
&lt;p&gt;I was fortunate enough to have attended Jeremy Howard's awesome &lt;a href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-one"&gt;deep learning certification program&lt;/a&gt; at University of San Francisco. One of the many insightful things Jeremy said was, and I paraphrase, &lt;a href="https://youtu.be/1-NYPQw5THU?t=1h19m11s"&gt;"In real life, I always start with a variable importance plot"&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Being just smart enough to recognize when smarter people have smart ideas, this is exactly what I did.&lt;/p&gt;
&lt;p&gt;I built a straightforward Random Forest in scikit-learn (using CV to select hyperparameters and using general best procedures) and retrieved this plot:&lt;/p&gt;
&lt;p&gt;&lt;img alt="VIP" src="https://github.com/mobbSF/blog/blob/master/images/VIP.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;The plot doesn't have a strong inflection point, making it difficult to decide where to draw a line in the sand about what is important to include. Let's see if we can deduce where this cutoff might fall, by first let's look at what isn't important:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We see that the number of Draw outcomes isn't important, which makes sense, as these are rare outcomes and are fairly irrelevant. The number of draws a boxer has doesn't have much bearing on the rest of their records.  &lt;/li&gt;
&lt;li&gt;We see that the permutations of stances across opponents (complimentary or opposite, &lt;a href="https://en.wikipedia.org/wiki/Southpaw_stance"&gt;southpaw&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Orthodox_stance"&gt;orthodox&lt;/a&gt;) isn't relevant.  &lt;/li&gt;
&lt;li&gt;Finally, the indicator columns aren't useful, which isn't a major surprise. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, let's look at what &lt;em&gt;is&lt;/em&gt; important. But first, let's take a little detour to think ahead about what might feel right. Let's ignore the variable importance plot for a moment.&lt;/p&gt;
&lt;p&gt;There is a fundamental axiom in boxing: "Hit and don't get hit." For boxers who fail to adhere to this basic principle, careers will be unnecesarily short, as physical damage sustained will quickly accumulate. So the idea of wear and tear on the body accumulated by a boxer over time seems like a naturally important component of what influences the outcome of a fight. &lt;/p&gt;
&lt;p&gt;This being considered, it should come as no great surprise that the most important feature in the Random Forest is &lt;code&gt;P1_days_since_ff&lt;/code&gt;. Recall, this variable reflects how long it has been since a boxer's career began. This perspective on career length turns out to be a natural fit.&lt;/p&gt;
&lt;p&gt;However, it's possible a boxer could fight once, then fight once again 10 years later. In this scenario,&lt;code&gt;P1_days_since_ff&lt;/code&gt; would be a value around 3,650, but this wouldn't be an accurate measurement of that boxer's accumulated wear and tear. The boxer would only have fought twice within that period. Fortunately for us, the next most important features on the plot corresponds to the aggregated number of rounds the opponents have fought. A long 'ring life' can clearly play a large role in a fighter's success (see &lt;a href="https://www.youtube.com/watch?v=Ja9iovR9B3E"&gt;Ali vs Holmes&lt;/a&gt; for a tragic example). Conversely, too few rounds can also play a role in predicting losing.&lt;/p&gt;
&lt;p&gt;Getting back to the variable importance plot, the two Quality of Opposition (&lt;code&gt;QOO&lt;/code&gt;) metrics are shown to be important, which is nice because they took a lot of effort to construct.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;last6_L&lt;/code&gt; and &lt;code&gt;last6_W&lt;/code&gt; variables have a large amounts of variance as evidenced by the bars. This most likely ties into the discussion about how records, as they stand alone, don't adequately reflect the quality of the opposition. For some boxers, the last 6 are very important; for some, they aren't. This makes sense.&lt;/p&gt;
&lt;p&gt;Content with the plot and with these features in mind, I experimented with various subsets of features and various configurations when fitting multilayer perceptrons in Keras.&lt;/p&gt;
&lt;p&gt;On a positive note, because of the small file size of the flat dataset (only 1.85 GB), there were no heavy demands on IO or memory. Thus I could rip through each epoch on a basic AWS cloud GPU painlessly, and iterate on models easily. &lt;/p&gt;
&lt;p&gt;Regarding the actual deep architecture of the MLP, I didn't have major overfitting issues, thus didn't get any advantage with a copious amount of dropout. Batch Normalization didn't give me any advantage, so I discarded it. Fundamentally, it's a remarkably simple dataset and most models I built performed very similarly. Epochs around 10 performed just fine.&lt;/p&gt;
&lt;p&gt;A decent final configuration looked like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mlp_004 = Sequential()
mlp_004.add(Dense(64, activation=&amp;#39;relu&amp;#39;, input_dim=13))
mlp_004.add(Dense(64, activation=&amp;#39;relu&amp;#39;))
mlp_004.add(Dense(64, activation=&amp;#39;relu&amp;#39;))
mlp_004.add(Dropout(0.2))
mlp_004.add(Dense(64, activation=&amp;#39;relu&amp;#39;))
mlp_004.add(Dense(1, activation=&amp;#39;sigmoid&amp;#39;))

mlp_004.compile(optimizer=&amp;#39;rmsprop&amp;#39;, loss=&amp;#39;binary_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])

mlp_004.fit(X_train, y_train, batch_size=64, validation_split=0.2, nb_epoch=10)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The validation set accuracy returned was 73%. &lt;/p&gt;
&lt;p&gt;Here's the confusion matrix:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CM" src="https://github.com/mobbSF/blog/blob/master/images/CM.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Here's the ROC curve:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="ROC" src="https://github.com/mobbSF/blog/blob/master/images/CM.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;OK, so how should I feel about such relatively modest scores, in this age of a solved MNIST, self-driving cars, and &lt;a href="https://www.cnbc.com/2017/08/11/elon-musk-issues-a-stark-warning-about-a-i-calls-it-a-bigger-threat-than-north-korea.html"&gt;Elon Musk's dire warnings&lt;/a&gt; of &lt;a href="https://www.youtube.com/watch?v=-WIwQlMesr0"&gt;Arnold coming baaack&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;I must admit, I feel pretty good about it. It's useful to take a step back here and reiterate that the purpose of this project is to make money gambling on boxing. If we were to use this algorithm to indicate when to place a bet, then we would prefer a larger precision at the expense of recall. What this means it that it's better to avoid betting and miss out on opportunities to win (lower recall), as long as we are more confident that when we &lt;em&gt;DO&lt;/em&gt; bet, we will win. More in the third post on these approaches.&lt;/p&gt;
&lt;p&gt;Meanwhile, allow me to wander a bit (yet again!) and discuss one of the many experiments I ran that didn't pay off. I went ahead and went for a moonshot. The reality is that as far as wagering on boxing go, it's one thing to wager on a W or L outcome. But if you can win a bet by predicting a more granular types of outcomes, the payouts are several orders of magnitude better. The actual type of outcome -- either a judges decision, or an actual knockout, or a technical knockout -- that's where the big bucks are. And when it comes to knockouts, if it's possible to predict the actual round? The payouts are huge. &lt;/p&gt;
&lt;p&gt;I changed the labels to represent the granular outcomes mentioned above, and I rebuilt the model and crossed my fingers. Unfortunately, and not too surprisingly, accuracy dropped to around 30%. Hummm...well...worth a shot. And definitely worth revisiting again later.&lt;/p&gt;
&lt;p&gt;Next up...prediction time!&lt;/p&gt;</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry><entry><title>Deep Learning for Sport Wagering Part 1 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-1-of-3.html" rel="alternate"></link><published>2017-09-15T00:00:00-07:00</published><updated>2017-09-15T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-09-15:/deep-learning-for-sport-wagering-part-1-of-3.html</id><summary type="html">&lt;p&gt;Characteristics of the dataset&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 1: Characteristics of the dataset&lt;/h4&gt;
&lt;p&gt;Not long ago, I was reading Nate Silver's blog, where there was some discussion about basketball. In particular, my hometown's team, the Golden State Warriors. At the time of the writing, the Warriors were surging towards the status of present-day dynasty, and the blog post was examining ways that the team performed that were revolutionary in the sport.&lt;/p&gt;
&lt;p&gt;One particular line in &lt;a href="https://fivethirtyeight.com/features/how-the-golden-state-warriors-are-breaking-the-nba/"&gt;the blog post&lt;/a&gt; stuck out for me: "It’s as if at some point in the past few years, the Warriors solved contemporary basketball..."&lt;/p&gt;
&lt;p&gt;Solved? A bit heavy on the hyperbole, but maybe not too far off.&lt;/p&gt;
&lt;p&gt;This got me thinking about other sports, and their capacity to be understood via data science, analytics, and deep learning. In particular, I became interested in the somewhat marginal and obscure (by major American sport standards, anyway) sport of boxing. I began to think that boxing could lend itself to being 'solved' nicely, because it has many characteristics that lend it to straightforward analysis and modeling.&lt;/p&gt;
&lt;p&gt;At it's heart, boxing has a simple structure.  Unlike many popular sports, it's not a team sport -- so there isn't a dynamic interplay between multiple individuals. It's also not new: the sport became standardized in the late 1600s, via the &lt;a href="https://en.wikipedia.org/wiki/Marquess_of_Queensberry_Rules"&gt;Marquess of Queensberry Rules&lt;/a&gt;. Thus, there is plenty of data available.  Fortunately for me, much of it is available online.&lt;/p&gt;
&lt;p&gt;To make the project more impactful, I decided to set a very specific goal. I've found that often, personal projects such as these will hold up better if there is the possibility of a good payoff at the end --  so I figured, why not try to build an algorithm that would allow me to win money in Vegas? Thus I set the goal of creating a tool for successful wagering on boxing.&lt;/p&gt;
&lt;p&gt;After quite a few long nights and weekends of collecting, reshaping, and modeling the data, I came into posession a pretty good model. The tl;dr is that the model guarantees a result of up to an (fill this in soon) &lt;insert&gt; return on investment. &lt;/p&gt;
&lt;p&gt;So, if you make it through these (hopefully not painful to read) blog posts, then go ahead and check on some upcoming fights. Ask me where to put your money, and I might be able to provide you with the 'nap.' In case you didn't know, the word 'nap' refers to a highly confident bet. That's been a fun side-effect from this project -- I learned a lot of cool new slang terms! &lt;/p&gt;
&lt;p&gt;There will be 3 parts to this blog post:&lt;/p&gt;
&lt;p&gt;1) Characteristics of the data&lt;br&gt;
2) Building models&lt;br&gt;
3) Prediction and Evaluation  &lt;/p&gt;
&lt;h3&gt;Characteristics of the data&lt;/h3&gt;
&lt;p&gt;The project was built on a very substantial dataset. The were two major sources of data. First, I aquired metadata on 373,415 individual boxers. Second, I had a collection of over 3.5 million fights (3,529,624 to be exact). These I imported into two MySQL tables. The dataset spanned the entire history of the sport. It was really fun to dig into. There were fighters from every corner of the globe, competing from the very beginning of the sport to the present day. There were boxers in every weight class from minimumweight upward, possessing all skill levels, at all ages, and exhibiting all levels of success. Perusing revealed some quite obscure fighters: a boxer from Accra, Ghana, who fought only once (unfortunately losing by knockout), back in 1966. Then, there was data on all the modern day multimillionaire heavyweight champions. There were was, for example, &lt;a href="https://en.wikipedia.org/wiki/Wladimir_Klitschko"&gt;Wladamir Klitchko&lt;/a&gt;, nicknamed 'Dr Steelhammer.' &lt;/p&gt;
&lt;p&gt;The dataset definitely possessed depth. The breadth in features for the first source of data, was as such:&lt;/p&gt;
&lt;h5&gt;boxer data&lt;/h5&gt;
&lt;p&gt;name&lt;br&gt;
sex&lt;br&gt;
birth_date&lt;br&gt;
division&lt;br&gt;
stance&lt;br&gt;
height&lt;br&gt;
reach&lt;br&gt;
country&lt;br&gt;
residence&lt;br&gt;
birth_place&lt;br&gt;
world_rank&lt;br&gt;
total_wins&lt;br&gt;
ko_wins&lt;br&gt;
total_losses&lt;br&gt;
ko_losses&lt;br&gt;
draws&lt;br&gt;
rounds&lt;br&gt;
ko_percent  &lt;/p&gt;
&lt;p&gt;Although elements like a boxer's weight are extremely important in real life boxing, some particular features like that and others were not applicable.
For example, Manny Pacquiao's first fight was at 98 pounds, but his most recent fight was at 146. So a single value pulled from the metadata would only reflect the most recent weight, not the weight at each fight.&lt;/p&gt;
&lt;p&gt;It was simple to keep stance (orthodox or southpaw) as a categorical variable.&lt;/p&gt;
&lt;p&gt;The dataset was balanced: 55% Wins, 45% Losses.&lt;/p&gt;
&lt;p&gt;More useful was the second source of data, the specific data for each boxing match:  &lt;/p&gt;
&lt;h5&gt;fight data&lt;/h5&gt;
&lt;p&gt;boxer1_id&lt;br&gt;
boxer2_id&lt;br&gt;
date&lt;br&gt;
location&lt;br&gt;
rounds_planned&lt;br&gt;
rounds_happened&lt;br&gt;
boxer1_mass&lt;br&gt;
boxer2_mass&lt;br&gt;
boxer2_wins&lt;br&gt;
boxer2_loses&lt;br&gt;
boxer2_draws&lt;br&gt;
boxer2_last6_wins&lt;br&gt;
boxer2_last6_loses&lt;br&gt;
boxer2_last6_draws&lt;br&gt;
outcome&lt;br&gt;
outcome_type&lt;br&gt;
rating&lt;br&gt;
time&lt;br&gt;
referee&lt;br&gt;
judge1&lt;br&gt;
judge2&lt;br&gt;
judge3&lt;br&gt;
judge1_score&lt;br&gt;
judge2_score&lt;br&gt;
judge3_score&lt;br&gt;
titles&lt;br&gt;
comments  &lt;/p&gt;
&lt;p&gt;As with the metadata for the boxer, I discarded some features of the fights. Much of it was nice but not functionally applicable, such as the names of the referee and judges, and comments, etc.&lt;/p&gt;
&lt;p&gt;With that being settled, the first important decision I made with respect to transformation of the data was to do a self-join within the fight table in MySQL. Thus, as shown above, each record in the dataset represented one fight. There would be a boxer1 and a boxer2. The target for each row would be Win, Lose, or Draw, with respect to boxer1. There would also be the granular outcome information: the type of W, L or D. After all, there are many ways for a boxing match to end: points, knockout, disqualification, waved off via accidental headbut, quitting on the stool, etc, etc.&lt;/p&gt;
&lt;p&gt;(Quick note on a detail of the sport itself: Notice that although boxing is an individual sport, each fighter also has a whole team behind them. During competition, there is a coach, obviously, and there is the &lt;a href="https://en.wikipedia.org/wiki/Cutman"&gt;cutman&lt;/a&gt; as well. The cutman handles the first aid between rounds. These two team members are called the 'secondaries' -- so in boxing, we'll call the 2 boxers 'primaries'. I quickly renamed boxer1 and boxer2 to P1 and P2.)&lt;/p&gt;
&lt;p&gt;Looking at these data, all seemed promising. But after some more reflection, I started to be concerned with what this rectangular representation was missing.
Really, these fights are all sequences of events that happen for a boxer -- they start with fight #1, then continue forward until the end of their careers. This made me interested in engineering some temporal variables, which I proceed to do in Pandas. Now, each row had these features:  &lt;/p&gt;
&lt;p&gt;P1_ageAtFight&lt;br&gt;
P2_ageAtFight&lt;br&gt;
P1_rounds_fought&lt;br&gt;
P2_rounds_fought  &lt;/p&gt;
&lt;p&gt;These 4 variables are rather self-explanatory given their names.  &lt;/p&gt;
&lt;p&gt;Another engineered feature was basically 'career length', or how many days it had been since the boxer's debut. These became:  &lt;/p&gt;
&lt;p&gt;P1_days_since_ff&lt;br&gt;
P2_days_since_ff  &lt;/p&gt;
&lt;p&gt;where 'ff' is short for 'first fight'.&lt;/p&gt;
&lt;p&gt;Some boxers were missing a birthdate, so I imputed these birthdates by assuming that each boxer's debut was on their 20th birthday. This was a simple subtraction from of 20 years from the date of the first fight.&lt;/p&gt;
&lt;p&gt;But beyond the sequential, temporal perspective, there was still something more to be done. I realized this dataset could also be realized as a graph, with nodes for the boxers and the fights, and nodes for their professional W, L, D records at the time of the fight. So I exported my dataset out of SQL and into Neo4J.&lt;/p&gt;
&lt;p&gt;Once this was done (and it took quite some time, given I had never used Neo4J before), I had a new way of conceptualizing these competitions. The schema is, in ASCI art:&lt;/p&gt;
&lt;p&gt;(A boxer, call them 'P1')--[had a record (W, L, D) on some date] --&amp;gt;(and there was a Fight at some location, with some outcome).&lt;/p&gt;
&lt;p&gt;Now, going the other direction,  &lt;/p&gt;
&lt;p&gt;(and there was a Fight at some location, with some outcome)&amp;lt;--[the opponent had a record, (W, L, D) on that same date]---(The opponent, call them 'P2')&lt;/p&gt;
&lt;p&gt;This actually is more clear when you look at the actual Neo4J graph itself:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph" src="https://github.com/mobbSF/blog/blob/master/images/Neo_000.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Here you see the blue boxer nodes, their records at a given time (red node), the fight node (green), and the complementary information for their opponents. Note that this particular boxer had a few rematches which are visible when two edges touch the same blue P2 (P2 being the opponent), node.&lt;/p&gt;
&lt;p&gt;Actually, it's interesting to see how the graph can be expanded. Take a look a Muhammad Ali's and his career:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 1" src="https://github.com/mobbSF/blog/blob/master/images/Neo_001.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;and here's &lt;a href="https://www.youtube.com/watch?v=-FZBzGhxERg"&gt;Ali's fight with Archie Moore&lt;/a&gt;, and Archie Moore's career: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 2" src="https://github.com/mobbSF/blog/blob/master/images/Neo_002.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;...and here's &lt;a href="https://www.youtube.com/watch?v=MUT71-jyY2s"&gt;Archie Moore's fight with Jimmy Slade&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 3" src="https://github.com/mobbSF/blog/blob/master/images/Neo_003.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;and so forth.&lt;/p&gt;
&lt;p&gt;Envisioned as a graph, it is insigtful to see the interconnectedness of the sport as a whole. &lt;/p&gt;
&lt;p&gt;Now that the Neo4J database was built, the first metric I focused on creating using Cypher was a 'quality of opposition' (QOO) score.&lt;/p&gt;
&lt;p&gt;QOO is necessary to suss out boxers with inflated records. Interestingly enough, boxing is the only sport (that I can think of, anyway) where a boxer actually gets to choose their opponent. There are no tournaments...no leagues...just arrangements between two boxers and the businesspeople around them, to hold an event. So what is stopping a boxer from inflating their record with &lt;a href="http://boxrec.com/en/boxer/4741"&gt;lousy opposition&lt;/a&gt;? Not much.&lt;/p&gt;
&lt;p&gt;It all really comes down to the quality of a boxer's opponents. If a boxer (say P1) has just been beating up on &lt;a href="https://en.wikipedia.org/wiki/Tomato_can_(sports_idiom)"&gt;tomato cans&lt;/a&gt;, then we need to acknowledge this. 
Because if P2 was battling top-quality opposition, then you'd be wise to put your money on P2. Because they both could have records of 20 Wins, 0 losses.&lt;/p&gt;
&lt;p&gt;(In reality, most successful fighters start competing relatively frequently, against somewhat weak opposition. Later, they increase the quality of opposition as they decrease the frequency of competition. So the above scenario is an exaggeration, in most cases).&lt;/p&gt;
&lt;p&gt;To show how the metric works, let's first start with this made up, simplified visual scenario:&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 1" src="https://github.com/mobbSF/blog/blob/master/images/QOO_001.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;As you can see, P1 has fought 3 opponents (they live in 'Layer 1'), and each of those opponents had fought 3 opponents themselves (they live in 'Layer 2').&lt;/p&gt;
&lt;p&gt;Now consider Layer 2. For each group of 3 fights, sum up these fighters' records as : count(Wins) / count(fights).&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 2" src="https://github.com/mobbSF/blog/blob/master/images/QOO_002.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Now, recursively running back up the graph, let's see how the boxer in Layer 1 performed against this group. This will be count(Wins) / count(opponents) from above, but now multiplied by the previously calculated value. Voila; we have the QOO metric.&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 3" src="https://github.com/mobbSF/blog/blob/master/images/QOO_003.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;After setting a QOO score for all fights on the nodes in Neo4J, it was easy to put together another quick and dirty metric: QOOP, which for lack of better nomenclature, is 'Quality of opposition prime'. Here, I just took the mean of all a boxer's opponents' QOOs and wrote it to the record node.&lt;/p&gt;
&lt;p&gt;Indicator columns with 0 or 1 were included, in case the QOO metrics couldn't be built. This could happen if, say, a fight was a boxer's debut.&lt;/p&gt;
&lt;p&gt;As a work in progress, I'm still playing with different ways to extract value from the Neo4J implementation. But this was a good start and took me pretty far.&lt;/p&gt;
&lt;p&gt;Now that the data were all cleaned up ready to go, it was time to (finally!!) get to the fun part...building some deep learning models.&lt;/p&gt;</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry></feed>