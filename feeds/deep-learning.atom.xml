<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Matt O'Brien (dot) Me - deep learning</title><link href="http://mattobrien.me/" rel="alternate"></link><link href="http://mattobrien.me/feeds/deep-learning.atom.xml" rel="self"></link><id>http://mattobrien.me/</id><updated>2017-10-09T00:00:00-07:00</updated><entry><title>Deep Learning for Sport Wagering Part 3 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-3-of-3.html" rel="alternate"></link><published>2017-10-09T00:00:00-07:00</published><updated>2017-10-09T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-10-09:/deep-learning-for-sport-wagering-part-3-of-3.html</id><summary type="html">&lt;p&gt;Predicting&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 3: Prediction and Evaluation&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-1-of-3.html"&gt;Part 1: Characteristics of the dataset&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-2-of-3.html"&gt;Part 2: Modeling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I considered one major betting strategy during this final phase of the project. It is as follows:  &lt;/p&gt;
&lt;p&gt;If&lt;br /&gt;
$\text{predicted probability} &amp;gt; \text{some decision threshold}$, 
 and&lt;br /&gt;
$\text{predicted probability} &amp;gt; \text{sportsbook probability}$&lt;br /&gt;
then place bet  &lt;/p&gt;
&lt;p&gt;This strategy can be thought of as conservative approach. We include a parameter that indicates if we are confident that we have an edge on the casino or sportsbook or not.  &lt;/p&gt;
&lt;p&gt;Allow me a quick foray into the structure of gambling. Sportsbook odds are set, not by information, but by popular sentiment as it is revealed by &lt;a href="https://www.docsports.com/gambling-terms.html"&gt;action&lt;/a&gt;. If some Boxer A gets a large amount of action, then the sportsbook will consider Boxer A to be more likely to win, and consequently, payout on this outcome is reduced. Thus, my model is attempting to answer the question, "When does prediction based on historic data result in a confidence higher than the confidence of the sportsbook -- which is a function of popular sentiment?". In this sense, at it's most stripped down, the model is trying make a totally impartial, data driven decision, and looks for opportunities when public perception is not aligned with historically based signal.  &lt;/p&gt;
&lt;p&gt;The strategy also has the opportunity to benefit from it's built-in safeguard. Suppose the sportsbook places some Boxer A at a 10% chance of winning, and the model predicts an 11% chance of winning. Without the safeguard, the model would decide to place the bet. Wagering on such low probabilities is something we don't want. Instead, we want to see action whenever the algorithm is confident above some appropriate threshold.&lt;/p&gt;
&lt;p&gt;To be implemented, first comes acquistion of more data. Historic sportsbook odds needed to be collected so that we could compare them with model's.  &lt;/p&gt;
&lt;p&gt;In boxing, the bookmaker's odds come structured into a form which is referred to as the &lt;a href="https://en.wikipedia.org/wiki/Odds#Moneyline_odds"&gt;moneyline&lt;/a&gt;. The moneyline is a little confusing at first. Generally, one fighter who considered favored to win is assigned a negative number. The other fighter is considered the underdog, and is assigned a positive number. &lt;/p&gt;
&lt;p&gt;The best way to remember how to read the moneyline is to always start with the image of a one-hundred dollar bill in your mind.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A negative number (assocated with the favored fighter) shows how much money you need to bet to win a profit of $100.  &lt;/li&gt;
&lt;li&gt;A positive number (associated with the underdog) shows how much profit a winning wager of $100 would yield.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if the moneyline has Boxer A at -130, we know that Boxer A is expected to win. Further, you know you'd have to place a \$130 bet on this fighter to win \$100.&lt;br /&gt;
For Boxer B, the moneyline might have them set at +110. This mean Boxer B is the underdog, and if you placed a \$100 bet on this fighter, you'd win \$110.  &lt;/p&gt;
&lt;p&gt;Since Keras is returning the probability of a boxer winning, we now need to convert the moneyline into regular probabilities so we can compare apples to apples.  &lt;/p&gt;
&lt;p&gt;To do this, first the actual numeric values (-130 and +110, on this example above), must be converted to what are referred to as &lt;a href="https://www.sbo.net/strategy/implied-probability/"&gt;implied probabilities&lt;/a&gt; (more about implied probabilities in a second). The formula is as follows:  &lt;/p&gt;
&lt;p&gt;$\text{Implied probability for 'negative' moneyline} = \frac{ - ( \text{'negative' moneyline value})}{- ( \text{'negative' moneyline value} ) + 100}$&lt;br /&gt;
and  &lt;/p&gt;
&lt;p&gt;$\text{Implied probability for 'positive' moneyline} = \frac{100}{\text{'positive' moneyline value} + 100}$&lt;/p&gt;
&lt;p&gt;Thus, -130 is converted to 0.56, and +110 is converted to 0.50.  &lt;/p&gt;
&lt;p&gt;But what is an implied probability anyway?&lt;/p&gt;
&lt;p&gt;Implied probability is our usual notion of probability which has actually been modified by what is called either &lt;a href="https://en.wikipedia.org/wiki/Vigorish"&gt;vigorish, or juice&lt;/a&gt;. Both of these terms refer to a built-in edge, by the bookmaker, on the true odds. The modification shifts the moneyline is such a way that the sportsbooks can make their profit. Usually, the vig amounts to 20 points. It's basically the casino's cut. Fortunately, it's easy to remove the vigorish using this simple formula:  &lt;/p&gt;
&lt;p&gt;Take one of your implied probability. Divide it by the sum of both of your implied probabilities.  &lt;/p&gt;
&lt;p&gt;Thus:  &lt;/p&gt;
&lt;p&gt;$\text{Actual probability } = \frac{\text{Implied probability A}}{\text{Implied probability A} + \text{Implied probability B}}$&lt;/p&gt;
&lt;p&gt;With the math settled, I began searching for a set of historic moneylines for records which I could use in my test set. Using a variety of sources (including laborious searching of the Wayback Machine, and locating an actual broker for assistance), I collected a set of 728 moneylines. After munging, the final size was 679.  &lt;/p&gt;
&lt;p&gt;We now bring our attention back to decision thresholds. What would be the optimal value where our $\text{model probability} &amp;gt; \text{some decision threshold}$?  To determine this, it was merely a matter of looping through thresholds from $[ 0, 1 ]$ by 0.1 and collecting the resulting classifications.  &lt;/p&gt;
&lt;p&gt;The outputs collects at each of these varying decision thresholds were as follows:   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Total number of wagers that satisified the criteria and thus were placed  &lt;/li&gt;
&lt;li&gt;Number of wagers placed which won  &lt;/li&gt;
&lt;li&gt;Number of wagers placed which lost  &lt;/li&gt;
&lt;li&gt;A tabulation of the balance resulting from money won via successful wagers and money lost via unsuccessful wagers  &lt;/li&gt;
&lt;li&gt;ROI (return on investment): simply $\frac{\text{balance}}{\text{total investment}}$  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We assumed that each bet placed was a \$100 bet. Every loss will incur a deduction of \$100, whereas each winning bet will earn a deposit depending on the sportsbook odds. This means if the model can predict 'easy' matches, it can win a smaller amount of money, but if the matches are harder to predict, the model can earn more.  &lt;/p&gt;
&lt;p&gt;Here is a plot showing the outcome for #1 on the list above:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="wagers" src="https://github.com/mobbSF/blog/blob/master/images/wagers.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;This gives us an intuition on where to place our decision threshold. We are interested in the point where the blue line and the green line are closest together, which means we will win the highest proportion of our placed bets. Simultaneously we would like this point to be as high as possible along the y axis, meaning the model chose to place a high net number of bets. &lt;/p&gt;
&lt;p&gt;The chart below shows the model will place the proportionately largest number of winning bets around an 0.85 to 0.94 decision threshold.  &lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/mobbSF/blog/blob/master/images/chart_002.png" width="200"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="chart_001" src="https://github.com/mobbSF/blog/blob/master/images/chart_001.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;It looks like roughly 0.90 is a reasonable place to set the threshold.  &lt;/p&gt;
&lt;p&gt;Now that we've got a feel for where our threshold might be, we can look forward to the bottom line -- did we turn a profit?  &lt;/p&gt;
&lt;p&gt;Here is a plot showing the outcome for #5 (ROI) on the list above:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="ROI" src="https://github.com/mobbSF/blog/blob/master/images/ROI.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;This chart shows the ROI values at the regions of interest we saw in the first plot:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="chart_002" src="https://github.com/mobbSF/blog/blob/master/images/chart_002.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;This chart shows we can get an ROI of roughly 22.5% if we set the decision threshold to 0.90. We could push it up to 24.8% if we choose 0.94 as the threshold, but notice the precipitous drop starting at 0.95 on the plot above. Better to be wary of the presence of variance and/or noise and choose to focus on a more median value.&lt;/p&gt;
&lt;p&gt;To unpack the ROI value, we can look at the actual dollar amount earned:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="cash" src="https://github.com/mobbSF/blog/blob/master/images/cash.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;We are seeing that when we stick with 0.90 we earn \$292.00, from a net investment of \$1,300.&lt;/p&gt;
&lt;p&gt;Finally, let's see what the plot looks like when we view most of these results simultaneously:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="functions" src="https://github.com/mobbSF/blog/blob/master/images/functions.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;Here we can see the sweet spot of 0.90 clearly. &lt;/p&gt;
&lt;p&gt;However, modeling like this isn't always deterministic. Small variations in inputs such that occur during randomly selecting test and validation sets can ripple through the pipeline and give different results.  &lt;/p&gt;
&lt;p&gt;I reran the project from top to bottom a few times, and found that indeed the threshold is a pretty fragile spot. These three images show that the 0.9 threshold might not always be the optimum.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="functions" src="https://github.com/mobbSF/blog/blob/master/images/slate.png?raw=true" /&gt; &lt;/p&gt;
&lt;p&gt;How to mitigate this? I decided to create 100 models, evaluate each of them, and take averages.  &lt;/p&gt;
&lt;p&gt;To visually get a feel for the variance, I made a plot of these 100 models:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="functions" src="https://github.com/mobbSF/blog/blob/master/images/cash_100.png?raw=true" /&gt; &lt;/p&gt;
&lt;p&gt;Quite a lot of varaiance. Here's a plot of the average:&lt;/p&gt;
&lt;p&gt;&lt;img alt="functions" src="https://github.com/mobbSF/blog/blob/master/images/functions_100_mean.png?raw=true" /&gt; &lt;/p&gt;
&lt;p&gt;It seems like the choice of 0.90 is still pretty conservative. But in gambling, perhaps conservative is okay.&lt;/p&gt;
&lt;p&gt;With the bulk of the work now done, we can claim success.  &lt;/p&gt;
&lt;p&gt;The outcome can be summed up in this elevator pitch sized statement:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"The model, with a decision threshold of 0.90, chose to place thirteen bets, winning all but one. With an initial investment of 
\$1,300, it won \$292, which represents a ROI of 22.5%."&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;This system performed much better than I expected. My initial hope was simply to build a MLP which had accuracy better than a coinflip. But the fact that the model can turn a profit when swimming with the Vegas sharks is very exciting.&lt;/p&gt;
&lt;p&gt;The next step is to put the model to use, see how it performs over a few month time period, and then see what improvements can be made. This is the true test -- putting my real money where my mouth is! I will report back with a Part 4 of this blog post series when I have had enough wagering experience to infer what can be improved. &lt;/p&gt;
&lt;p&gt;Thank you for reading this far. Please comment if you have the inclination!&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry><entry><title>Deep Learning for Sport Wagering Part 2 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-2-of-3.html" rel="alternate"></link><published>2017-09-16T00:00:00-07:00</published><updated>2017-09-16T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-09-16:/deep-learning-for-sport-wagering-part-2-of-3.html</id><summary type="html">&lt;p&gt;Modeling&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 2: Modeling&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-1-of-3.html"&gt;Part 1: Characteristics of the dataset&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-3-of-3.html"&gt;Part 3: Modeling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I was fortunate enough to have attended Jeremy Howard's awesome &lt;a href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-one"&gt;deep learning certification program&lt;/a&gt; at University of San Francisco. One of the many insightful things Jeremy said was, and I quote verbatium, &lt;a href="https://youtu.be/1-NYPQw5THU?t=1h19m11s"&gt;"The first thing I do, is try to get a feature importance plot printed."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Being just smart enough to recognize when smarter people have smart ideas, this is exactly what I did.&lt;/p&gt;
&lt;p&gt;I built a straightforward Random Forest in scikit-learn, using CV to select hyperparameters and using general best procedures. I retrieved this plot:&lt;/p&gt;
&lt;p&gt;&lt;img alt="VIP" src="https://github.com/mobbSF/blog/blob/master/images/VIP.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;The plot doesn't have a strong inflection point, making it difficult to decide where to draw a line in the sand about what is important to include and what isn't. Let's see if we can deduce where this cutoff might fall, by first let's look at what isn't important:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We see that the number of Draw outcomes isn't important, which makes sense, as these are rare outcomes and are fairly irrelevant. The number of draws a boxer has doesn't have much bearing on the rest of their records.  &lt;/li&gt;
&lt;li&gt;We see that the permutations of stances across opponents (complimentary or opposite, &lt;a href="https://en.wikipedia.org/wiki/Southpaw_stance"&gt;southpaw&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Orthodox_stance"&gt;orthodox&lt;/a&gt;) isn't relevant.  &lt;/li&gt;
&lt;li&gt;Finally, the indicator columns aren't useful, which isn't a major surprise. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, let's look at what &lt;em&gt;is&lt;/em&gt; important. But first, let's take a little detour to think ahead about what might feel right. Let's ignore the variable importance plot for a moment.&lt;/p&gt;
&lt;p&gt;There is a fundamental axiom in boxing: "Hit and don't get hit." For boxers who fail to adhere to this basic principle, careers will be unnecesarily short, as physical damage sustained will quickly accumulate. So the idea of wear and tear on the body accumulated by a boxer over time seems like a naturally important component of what influences the outcome of a fight. &lt;/p&gt;
&lt;p&gt;This being considered, it should come as no great surprise that the most important feature in the Random Forest is &lt;code&gt;P1_days_since_ff&lt;/code&gt;. Recall, this variable reflects how long it has been since a boxer's career began. This perspective on career length turns out to be a natural fit.&lt;/p&gt;
&lt;p&gt;However, it's possible a boxer could fight once, then fight once again 10 years later. In this scenario,&lt;code&gt;P1_days_since_ff&lt;/code&gt; would be a value around 3,650, but this wouldn't be an accurate measurement of that boxer's accumulated wear and tear. The boxer would only have fought twice within that period. Fortunately for us, the next most important features on the plot corresponds to the aggregated number of rounds the opponents have fought. A long 'ring life' can clearly play a large role in a fighter's success (see &lt;a href="https://www.youtube.com/watch?v=Ja9iovR9B3E"&gt;Ali vs Holmes&lt;/a&gt; for a tragic example). Conversely, too few rounds can also play a role in predicting losing.&lt;/p&gt;
&lt;p&gt;Getting back to the variable importance plot, the two Quality of Opposition (&lt;code&gt;QOO&lt;/code&gt;) metrics are shown to be important, which is nice not at the very least because they took a lot of effort to construct!&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;last6_L&lt;/code&gt; and &lt;code&gt;last6_W&lt;/code&gt; variables have a large amounts of variance as evidenced by the bars. This most likely ties into the discussion about how records, as they stand alone, don't adequately reflect the quality of the opposition. For some boxers, the last 6 are very important; for some, they aren't. This makes sense.&lt;/p&gt;
&lt;p&gt;Content with the plot and with these features in mind, I experimented with various subsets of features and various configurations when fitting multilayer perceptrons in Keras.&lt;/p&gt;
&lt;p&gt;On a positive note, because of the small file size of the flat dataset (only 1.85 GB), there were no heavy demands on IO or memory. Thus I could rip through each epoch on a basic AWS cloud GPU painlessly, and iterate on models easily. &lt;/p&gt;
&lt;p&gt;Regarding the actual deep architecture of the MLP, I didn't have major overfitting issues, thus didn't get any advantage with a copious amount of dropout. Batch Normalization didn't give me any advantage, so I discarded it. Fundamentally, it's a remarkably simple dataset and most models I built performed very similarly. Epochs around 10 performed just fine.&lt;/p&gt;
&lt;p&gt;A decent final configuration looked like this: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mlp_004 = Sequential()
mlp_004.add(Dense(64, activation='relu', input_dim=13))
mlp_004.add(Dense(64, activation='relu'))
mlp_004.add(Dense(64, activation='relu'))
mlp_004.add(Dropout(0.2))
mlp_004.add(Dense(64, activation='relu'))
mlp_004.add(Dense(1, activation='sigmoid'))

mlp_004.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

mlp_004.fit(X_train, y_train, batch_size=64, validation_split=0.2, nb_epoch=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The validation set accuracy returned was 73%. &lt;/p&gt;
&lt;p&gt;Here's the confusion matrix:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CM" src="https://github.com/mobbSF/blog/blob/master/images/CM.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;Here's the ROC curve:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="ROC" src="https://github.com/mobbSF/blog/blob/master/images/ROC.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;OK, so how should I feel about such relatively modest scores, in this age of a solved MNIST, self-driving cars, and &lt;a href="https://www.cnbc.com/2017/08/11/elon-musk-issues-a-stark-warning-about-a-i-calls-it-a-bigger-threat-than-north-korea.html"&gt;Elon Musk's dire warnings&lt;/a&gt; of &lt;a href="https://www.youtube.com/watch?v=-WIwQlMesr0"&gt;Arnold coming baaack&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;I must admit, I feel pretty good about it. It's useful to take a step back here and reiterate that the purpose of this project is to make money gambling on boxing. If we were to use this algorithm to indicate when to place a bet, then we would prefer a larger precision at the expense of recall. What this means it that it's better to avoid betting and miss out on opportunities to win (lower recall), as long as we are more confident that when we &lt;em&gt;DO&lt;/em&gt; bet, we will win. More in the third post about this approach.&lt;/p&gt;
&lt;p&gt;Meanwhile, allow me to wander a bit (yet again!) and discuss one of the many experiments I ran that didn't pay off. I went ahead and went for a moonshot. The reality is that as far as wagering on boxing go, it's one thing to wager on a W or L outcome. But if you can win a bet by predicting a more granular types of outcomes, the payouts are several orders of magnitude better. The actual type of outcome -- either a judges decision, or an actual knockout, or a technical knockout -- that's where the big bucks are. And when it comes to knockouts, if it's possible to predict the actual round? The payouts are huge. &lt;/p&gt;
&lt;p&gt;I changed the labels to represent the granular outcomes mentioned above, and I rebuilt the model and crossed my fingers. Unfortunately, and not too surprisingly, accuracy dropped to around 30%. Hummm...well...worth a shot. And definitely worth revisiting again later.&lt;/p&gt;
&lt;p&gt;Next up...prediction time!&lt;/p&gt;</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry><entry><title>Deep Learning for Sport Wagering Part 1 of 3</title><link href="http://mattobrien.me/deep-learning-for-sport-wagering-part-1-of-3.html" rel="alternate"></link><published>2017-09-15T00:00:00-07:00</published><updated>2017-09-15T00:00:00-07:00</updated><author><name>Matt O'Brien</name></author><id>tag:mattobrien.me,2017-09-15:/deep-learning-for-sport-wagering-part-1-of-3.html</id><summary type="html">&lt;p&gt;Characteristics of the dataset&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Part 1: Characteristics of the dataset&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-2-of-3.html"&gt;Part 2 Modeling&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-3-of-3.html"&gt;Part 3: Prediction and Evaluation&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Not long ago, I was reading Nate Silver's blog, where there was some discussion about basketball. In particular, my hometown's team, the Golden State Warriors. At the time of the writing, the Warriors were surging towards the status of present-day dynasty, and the blog post was examining ways that the team performed that were revolutionary in the sport.&lt;/p&gt;
&lt;p&gt;One particular line in &lt;a href="https://fivethirtyeight.com/features/how-the-golden-state-warriors-are-breaking-the-nba/"&gt;the blog post&lt;/a&gt; stuck out for me: "It’s as if at some point in the past few years, the Warriors solved contemporary basketball..."&lt;/p&gt;
&lt;p&gt;Solved? A bit heavy on the hyperbole, but maybe not too far off.&lt;/p&gt;
&lt;p&gt;This got me thinking about other sports, and their capacity to be understood via data science, analytics, and deep learning. In particular, I became interested in the somewhat marginal and obscure (by major American sport standards, anyway) sport of boxing. I began to think that boxing could lend itself to being 'solved' nicely, because it has many characteristics that lend it to straightforward analysis and modeling.&lt;/p&gt;
&lt;p&gt;At it's heart, boxing has a simple structure.  Unlike many popular sports, it's not a team sport -- so there isn't a dynamic interplay between multiple individuals. It's also not new: the sport became standardized in the late 1600s, via the &lt;a href="https://en.wikipedia.org/wiki/Marquess_of_Queensberry_Rules"&gt;Marquess of Queensberry Rules&lt;/a&gt;. Thus, there is plenty of data available.  Fortunately for me, much of it is available online.&lt;/p&gt;
&lt;p&gt;To make the project more impactful, I decided to set a very specific goal. I've found that often, personal projects such as these will get more attention on the evenings and weekends if there is the possibility of a good payoff at the end --  so I figured, why not try to build an algorithm that would allow me to win money in Vegas? Thus I set the goal of creating a tool for successful wagering on boxing.&lt;/p&gt;
&lt;p&gt;After quite a long process of collecting, reshaping, and modeling a dataset, I came into posession a what I consider to be a very model. &lt;strong&gt;The tl;dr is that the model returned a 22.5% return on investment.&lt;/strong&gt; This is really exciting and shows the power of deep learning in a tangible manner. &lt;/p&gt;
&lt;p&gt;So, if you make it through these (hopefully not painful to read) blog posts, then I invite you to go ahead and check on some upcoming fights. Ask me where to put your money, and I might be able to provide you with the 'nap.' In case you didn't know, the term 'nap' refers to a highly confident bet. That's been a fun side-effect from this project -- I learned a lot of cool new slang. &lt;/p&gt;
&lt;p&gt;There will be 3 parts to this blog post:&lt;/p&gt;
&lt;p&gt;1) &lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-1-of-3.html"&gt;Characteristics of the dataset&lt;/a&gt;&lt;br /&gt;
2) &lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-2-of-3.html"&gt;Modeling&lt;/a&gt;
3) &lt;a href="http://www.mattobrien.me/deep-learning-for-sport-wagering-part-3-of-3.html"&gt;Prediction and Evaluation&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Part 1 is fairly dry and reflect the laborous truth that the majority of data science work is often dominated by collection and transforming data.  &lt;/p&gt;
&lt;p&gt;Part 2 is short and has some interesting forays into feature engineering vis-à-vis graph databases.  &lt;/p&gt;
&lt;p&gt;Part 3 is the most exciting, since we finally get to talk money!  &lt;/p&gt;
&lt;h3&gt;Characteristics of the data&lt;/h3&gt;
&lt;p&gt;The project was built on a very substantial dataset. The were two major sources of data. First, I aquired metadata on 373,415 individual boxers. Second, I had a collection of over 3.5 million fights (3,529,624 to be exact). Both were imported into MySQL tables. Interestingly, the dataset spanned the entire history of the sport. It was really fun to dig into. There were fighters from every corner of the globe, competing from the very beginning of the sport to the present day. There were boxers in every weight class from minimumweight upward, possessing all skill levels. They came in all ages, and exhibited all levels of success. Casually perusing revealed some quite obscure fighters: a boxer from Accra, Ghana, who fought only once back in 19066 (unfortunately losing by knockout). There was data on all the modern day multimillionaire champions. There were was, for example, &lt;a href="https://en.wikipedia.org/wiki/Wladimir_Klitschko"&gt;Wladamir Klitchko&lt;/a&gt;, nicknamed 'Dr Steelhammer.' The dataset definitely possessed depth. &lt;/p&gt;
&lt;p&gt;Looking specifically at the breadth of features for the metadata on each fighter, I had:&lt;/p&gt;
&lt;h5&gt;boxer data&lt;/h5&gt;
&lt;p&gt;name&lt;br /&gt;
sex&lt;br /&gt;
birth_date&lt;br /&gt;
division&lt;br /&gt;
stance&lt;br /&gt;
height&lt;br /&gt;
reach&lt;br /&gt;
country&lt;br /&gt;
residence&lt;br /&gt;
birth_place&lt;br /&gt;
world_rank&lt;br /&gt;
total_wins&lt;br /&gt;
ko_wins&lt;br /&gt;
total_losses&lt;br /&gt;
ko_losses&lt;br /&gt;
draws&lt;br /&gt;
rounds&lt;br /&gt;
ko_percent  &lt;/p&gt;
&lt;p&gt;At this point it was time to begin evaluating the features. Although elements like a boxer's weight are extremely important in real life boxing, this particular feature was not applicable because weight usually changes across a boxer's career. For example, Manny Pacquiao's first fight was at 98 pounds, but his &lt;a href="https://www.youtube.com/watch?v=OdvxQDVP4WI"&gt;most recent fight&lt;/a&gt; was at 146 pounds. So a single value pulled from the metadata table would only reflect the most recent weight, not the weight at each fight.&lt;/p&gt;
&lt;p&gt;It was simple to keep stance (orthodox or southpaw) as a categorical variable.&lt;/p&gt;
&lt;p&gt;The dataset was balanced: 55% Wins, 45% Losses.&lt;/p&gt;
&lt;p&gt;More useful was the second source of data, which was the specific data for each boxing match:  &lt;/p&gt;
&lt;h5&gt;fight data&lt;/h5&gt;
&lt;p&gt;boxer_id&lt;br /&gt;
date&lt;br /&gt;
location&lt;br /&gt;
rounds_planned&lt;br /&gt;
rounds_happened&lt;br /&gt;
boxer1_mass&lt;br /&gt;
boxer2_mass&lt;br /&gt;
boxer2_wins&lt;br /&gt;
boxer2_loses&lt;br /&gt;
boxer2_draws&lt;br /&gt;
boxer2_last6_wins&lt;br /&gt;
boxer2_last6_loses&lt;br /&gt;
boxer2_last6_draws&lt;br /&gt;
outcome&lt;br /&gt;
outcome_type&lt;br /&gt;
rating&lt;br /&gt;
time&lt;br /&gt;
referee&lt;br /&gt;
judge1&lt;br /&gt;
judge2&lt;br /&gt;
judge3&lt;br /&gt;
judge1_score&lt;br /&gt;
judge2_score&lt;br /&gt;
judge3_score&lt;br /&gt;
titles&lt;br /&gt;
comments  &lt;/p&gt;
&lt;p&gt;As with the metadata for the boxer, I discarded some features of the fights. Much of it was nice but not functionally applicable, such as the names of the referee and judges, and comments, etc.&lt;/p&gt;
&lt;p&gt;With that being settled, the first important decision I made with respect to transformation of the data was to do a self-join within the fight table in MySQL. Thus each record in the dataset represented one fight. There would be a Boxer 1 and a Boxer 2 . The target for each row would be Win, Lose, or Draw, with respect to Boxer 1. There would also be the granular outcome information: the type of W, L or D. After all, there are many ways for a boxing match to end: points, knockout, disqualification, waved off via accidental headbut, quitting on the stool, etc, etc.&lt;/p&gt;
&lt;p&gt;Quick note on a detail of the sport itself: Notice that although boxing is an individual sport, each fighter also has a whole team behind them. During competition, there is a coach in the corner, and there is also the &lt;a href="https://en.wikipedia.org/wiki/Cutman"&gt;cutman&lt;/a&gt;. The cutman handles the first aid between rounds. These two team members are called the 'secondaries' -- so in boxing, we'll refer to the 2 actual fighters 'primaries'. I quickly renamed Boxer 1 and Boxer 2 to P1 and P2.&lt;/p&gt;
&lt;p&gt;Looking at these data, all seemed promising. But after some more reflection, I started to be concerned with what this rectangular representation was missing.  &lt;/p&gt;
&lt;p&gt;In truth, these fights are all sequences of events that happen for a boxer -- they start with fight #1, then continue forward until the end of their careers. This made me interested in engineering 4 temporal variables, which I proceed to do in Pandas. Now, each row had these features:  &lt;/p&gt;
&lt;p&gt;P1_ageAtFight&lt;br /&gt;
P2_ageAtFight&lt;br /&gt;
P1_rounds_fought&lt;br /&gt;
P2_rounds_fought  &lt;/p&gt;
&lt;p&gt;These variables are rather self-explanatory given their names.  &lt;/p&gt;
&lt;p&gt;Another engineered feature was basically 'career length', or how many days it had been since the boxer's debut. These became:  &lt;/p&gt;
&lt;p&gt;P1_days_since_ff&lt;br /&gt;
P2_days_since_ff  &lt;/p&gt;
&lt;p&gt;where 'ff' is just short for 'first fight'.&lt;/p&gt;
&lt;p&gt;Some boxers were missing a birthdate, so I imputed these birthdates by assuming that each boxer's debut was on their 20th birthday. This was a simple subtraction from of 20 years from the date of the first fight.&lt;/p&gt;
&lt;p&gt;But beyond the sequential, temporal perspective, there was still something more to be done. I realized this dataset could also be realized as a graph, with nodes for the boxers and the fights. I could set nodes for the boxers' professional W, L, D records at the time of each fight. So I exported my dataset out of SQL and into Neo4J.&lt;/p&gt;
&lt;p&gt;Once this was done (and it took quite some time, given I had never used Neo4J before), I had a new way of conceptualizing these competitions. The schema is, in ASCI art:&lt;/p&gt;
&lt;p&gt;(A boxer, call them 'P1')--[had a record (W, L, D) on some date] --&amp;gt;(and there was a Fight at some location, with some outcome).&lt;/p&gt;
&lt;p&gt;Now, going the other direction,  &lt;/p&gt;
&lt;p&gt;(and there was a Fight at some location, with some outcome)&amp;lt;--[the opponent had a record, (W, L, D) on that same date]---(The opponent, call them 'P2')&lt;/p&gt;
&lt;p&gt;This actually is more clear when you look at the actual Neo4J graph itself:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph" src="https://github.com/mobbSF/blog/blob/master/images/Neo_000.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;Here you see the blue boxer nodes, their records at a given time (red node), the fight node (green), and the complementary information for their opponents. Note that this particular boxer had a few rematches which are visible when two edges touch the same blue P2 (P2 being the opponent), node.&lt;/p&gt;
&lt;p&gt;Actually, it's interesting to see how the graph can be expanded. Take a look a Muhammad Ali's and his career:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 1" src="https://github.com/mobbSF/blog/blob/master/images/Neo_001.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;and here's &lt;a href="https://www.youtube.com/watch?v=-FZBzGhxERg"&gt;Ali's fight with Archie Moore&lt;/a&gt;, and Archie Moore's career: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 2" src="https://github.com/mobbSF/blog/blob/master/images/Neo_002.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;...and here's &lt;a href="https://www.youtube.com/watch?v=MUT71-jyY2s"&gt;Archie Moore's fight with Jimmy Slade&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 3" src="https://github.com/mobbSF/blog/blob/master/images/Neo_003.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;and so forth.&lt;/p&gt;
&lt;p&gt;Envisioned as a graph, it is insightful to see the interconnectedness of the sport as a whole. Here's a few examples:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 4" src="https://github.com/mobbSF/blog/blob/master/images/Neo_004.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Neo4J graph 5" src="https://github.com/mobbSF/blog/blob/master/images/Neo_005.png?raw=true" /&gt;  &lt;/p&gt;
&lt;p&gt;Now that now that the Neo4J database was built, the first metric I focused on creating using Cypher was a 'quality of opposition' (QOO) score.&lt;/p&gt;
&lt;p&gt;QOO is necessary to suss out boxers with inflated records. Interestingly enough, boxing is the only sport (that I am aware of, at least) where a boxer actually gets to choose their opponent. There are no tournaments...no leagues...just arrangements between two boxers and the businesspeople around them, to hold an event. So what is stopping a boxer from inflating their record with &lt;a href="http://boxrec.com/en/boxer/4741"&gt;lousy opposition&lt;/a&gt;? Not much. And this happens often in practice.  &lt;/p&gt;
&lt;p&gt;It all really comes down to the quality of a boxer's opponents. If a boxer (say P1) has been beating up on &lt;a href="https://en.wikipedia.org/wiki/Tomato_can_(sports_idiom)"&gt;tomato cans&lt;/a&gt;, then we need to acknowledge this. 
Because if during that same time, another boxer (say P2) was battling top-quality opposition, then you'd be wise to put your money on P2. Because both P1 and P2 could have records of 20 Wins, 0 losses. In short, W L D records can be misleading.  &lt;/p&gt;
&lt;p&gt;(In reality, most successful fighters start competing relatively frequently, against somewhat weak opposition. Later, they increase the quality of opposition as they decrease the frequency of competition. So the above scenario is an exaggeration, in most cases).&lt;/p&gt;
&lt;p&gt;To show how the metric works, let's first start with this made up, simplified visual scenario:&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 1" src="https://github.com/mobbSF/blog/blob/master/images/QOO_001.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, P1 has fought 3 opponents (they live in 'Layer 1'), and each of those opponents had fought 3 opponents themselves (they live in 'Layer 2').&lt;/p&gt;
&lt;p&gt;Now consider Layer 2. For each group of 3 fights, sum up these fighters' records as : count(Wins) / count(fights).&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 2" src="https://github.com/mobbSF/blog/blob/master/images/QOO_002.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;Now, recursively running back up the graph, let's see how the boxer in Layer 1 performed against this group. This will be count(Wins) / count(opponents) from above, but now multiplied by the previously calculated value. Voila; we have the QOO metric.&lt;/p&gt;
&lt;p&gt;&lt;img alt="QOO 3" src="https://github.com/mobbSF/blog/blob/master/images/QOO_003.png?raw=true" /&gt;&lt;/p&gt;
&lt;p&gt;After setting a QOO score for all fights on the nodes in Neo4J, it was easy to put together another quick and dirty metric: QOOP, which for lack of better nomenclature, is 'Quality of opposition prime'. Here, I just took the mean of all a boxer's opponents' QOOs and wrote it to the record node.&lt;/p&gt;
&lt;p&gt;Indicator columns with 0 or 1 were included, in case the QOO metrics couldn't be built. This could happen if, say, a fight was a boxer's debut.&lt;/p&gt;
&lt;p&gt;There are still many other ways to extract value from the Neo4J implementation. For example, implementing an &lt;a href="https://en.wikipedia.org/wiki/Elo_rating_system"&gt;elo rating&lt;/a&gt; (as borrowed from chess) could result in a valuable datum for the state of each boxer during each match. But this was a good enough start and took me pretty far.&lt;/p&gt;
&lt;p&gt;Now that the data were all cleaned up ready to go, it was time to (finally!) get to the fun part...building some deep learning models.&lt;/p&gt;</content><category term="deep learning"></category><category term="sport"></category><category term="wagering"></category></entry></feed>